{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b73ee-38e3-4803-9501-0a4c0943e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Used to measure execution time of code segments\n",
    "import pandas as pd  # Primary data structure library for data manipulation and analysis\n",
    "import numpy as np  # Library for support to large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays\n",
    "from sqlalchemy import create_engine  # Database toolkit for Python, provides a way to create a connection to the database\n",
    "import re  # Library for regular expression operations, allows for text searching, matching, and manipulation\n",
    "from scipy import stats  # Module in SciPy library for statistical functions\n",
    "\n",
    "import nltk  # Natural Language Toolkit, library for symbolic and statistical natural language processing (NLP)\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])  # Downloads specific packages from NLTK for tokenization, lemmatization, and stopwords\n",
    "from nltk.tokenize import word_tokenize  # Function for tokenizing strings (splitting strings into words and punctuation)\n",
    "from nltk.corpus import stopwords  # Provides a list of 'stopwords' that can be filtered out from the text\n",
    "from nltk.stem import WordNetLemmatizer  # Class for lemmatizing words (reducing them to their base or root form)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV  # Functions and classes for splitting data, cross-validation, and hyperparameter tuning\n",
    "from sklearn.pipeline import Pipeline  # Class for creating a pipeline of transforms with a final estimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  # Classes for converting text to vector form and applying TF-IDF transformation\n",
    "from sklearn.decomposition import TruncatedSVD  # Class for dimensionality reduction using truncated singular value decomposition (SVD)\n",
    "from sklearn.multioutput import MultiOutputClassifier  # Strategy for multi-target classification\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision tree classifier\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random forest classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K-nearest neighbors classifier\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression classifier\n",
    "from sklearn.svm import SVC  # Support vector machine classifier\n",
    "from sklearn.metrics import hamming_loss, precision_score, recall_score, f1_score  # Functions for calculating common classification metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignores warnings to clean up output for readability\n",
    "\n",
    "engine = create_engine('sqlite:///dialogue.db')  # Creates a connection to the SQLite database file `dialogue.db`\n",
    "df = pd.read_sql('dialogue', engine)  # Loads the 'dialogue' table from the database into a DataFrame\n",
    "#display(df.head())\n",
    "print('Total # of exchanges: {}'.format(str(len(df))))  # Prints the total number of exchanges (rows) in the DataFrame\n",
    "\n",
    "genres = df['genres'].tolist()  # Converts the 'genres' column to a list\n",
    "genres = ','.join(genres)  # Joins all genre strings into a single string separated by commas\n",
    "genres = genres.split(',')  # Splits the single string back into a list of genres, effectively flattening the list\n",
    "genres = sorted(list(set(genres)))  # Removes duplicates and sorts the genres\n",
    "print('Count of unique genres: {}'.format(str(len(genres))))  # Prints the number of unique genres\n",
    "\n",
    "for genre in genres:\n",
    "    df[genre] = df['genres'].apply(lambda x: 1 if genre in x else 0)  # For each genre, adds a new column to the DataFrame where 1 indicates the genre is present in the movie's genres and 0 if not\n",
    "\n",
    "df['label_count'] = df[genres].sum(axis=1)  # Adds a new column 'label_count' to the DataFrame representing the total number of genres associated with each movie\n",
    "label_counts = df.groupby('label_count')['movie_id'].nunique().reset_index()  # Groups the DataFrame by 'label_count' and counts unique 'movie_id's for each group\n",
    "label_counts\n",
    "\n",
    "df_melt = pd.melt(df, id_vars='movie_id', value_vars=genres, var_name='genre', value_name='label')  # Transforms the DataFrame so each row corresponds to a movie-genre pair\n",
    "genre_counts = df_melt.groupby('genre')['label'].sum().reset_index()  # Groups the melted DataFrame by 'genre' and sums up the 'label' column to get the total count of each genre\n",
    "genre_counts.head()  # Displays the first few rows of the genre counts DataFrame\n",
    "\n",
    "\n",
    "X = df['dialogue']  # Assigns the 'dialogue' column to X, representing the input features for the model\n",
    "y = df[genres]  # Assigns the genre columns to y, representing the target labels for the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Splits the data into training and testing sets\n",
    "\n",
    "# tokenize data to be more suitable\n",
    "def tokenize(text):\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)  # Replaces all characters not in a-zA-Z0-9 with a space\n",
    "    tokens = word_tokenize(text)  # Tokenizes the cleaned text\n",
    "    lemmatizer = WordNetLemmatizer()  # Initializes the WordNet lemmatizer\n",
    "    clean_tokens = (lemmatizer.lemmatize(token).lower().strip() for token in tokens if token \\\n",
    "                    not in stopwords.words('english'))  # Lemmatizes, converts to lowercase, strips whitespace, and removes stopwords from the tokens\n",
    "    return clean_tokens  # Returns the cleaned tokens\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Setting steps for data processing\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD()),\n",
    "    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "    ])\n",
    "\n",
    "# Tunning Model by setting hyper-parameters\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],  # Specifies the range of n-values for different n-grams to be extracted by CountVectorizer\n",
    "    'clf__estimator__max_depth': [250, 500, 1000],  # Specifies the max depth for the decision tree classifier\n",
    "    'clf__estimator__min_samples_split': [1, 2, 6]  # Specifies the minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Cross Validation Training\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, scoring='f1_weighted', cv=4, n_jobs=1, verbose=10)  # Initializes a GridSearchCV object for hyperparameter tuning\n",
    "cv.fit(X, y)  # Fits the grid search to the data\n",
    "print('GridSearch complete.')  # Indicates that the grid search is complete\n",
    "print('Best params:')  # Prints the best parameter combination found\n",
    "print(cv.best_params_)  # Prints the actual best parameters\n",
    "print('Best score:')  # Prints the best score achieved with the best parameters\n",
    "print(cv.best_score_)  # Prints the actual best score\n",
    "print('Time elapsed:')  # Prints the time elapsed since the new start_time\n",
    "print(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time)))\n",
    "\n",
    "# function that utilize trained model for prediction on new data\n",
    "def predict_genres(text):\n",
    "    pred = pd.DataFrame(cv.predict([text]), columns=genres)  # Predicts the genres for a given text input\n",
    "    pred = pred.transpose().reset_index()  # Transposes the prediction DataFrame for easier manipulation\n",
    "    pred.columns = ['genre', 'prediction']  # Renames columns for clarity\n",
    "    predictions = pred[pred['prediction']==1]['genre'].tolist()  # Extracts the genres predicted as present (1)\n",
    "    return predictions  # Returns the list of predicted genres\n",
    "\n",
    "line1 = \"If god did not exist it would be necessary to invent him.\"  # Example line of dialogue for testing\n",
    "line2 = \"It's funny... the world is so different in the daylight. In the dark, your fantasies get so out of hand. \\\n",
    "But in the daylight everything falls back into place again.\"  # Another example line of dialogue for testing\n",
    "print('Line 1: {}'.format(predict_genres(line1)))  # Prints genres predicted for line1\n",
    "print('Line 2: {}'.format(predict_genres(line2)))  # Prints genres predicted for line2\n",
    "\n",
    "line  = ''  # Initializes an empty string\n",
    "while line != 'exit':  # Continues to prompt for input until 'exit' is entered\n",
    "    line = input(\"Enter a line of text: \")  # Prompts user for a line of text\n",
    "    print('Genre: {}'.format(predict_genres(line)))  # Prints the predicted genres for the entered text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
